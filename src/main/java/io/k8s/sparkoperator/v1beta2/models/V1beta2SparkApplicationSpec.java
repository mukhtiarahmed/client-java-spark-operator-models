/*
 * Kubernetes
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: v1.22.7
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package io.k8s.sparkoperator.v1beta2.models;

import java.util.Objects;
import java.util.Arrays;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateBatchSchedulerOptions;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateDeps;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateDriver;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateDynamicAllocation;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateExecutor;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateMonitoring;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateRestartPolicy;
import io.k8s.sparkoperator.v1beta2.models.V1beta2ScheduledSparkApplicationSpecTemplateVolumes;
import io.k8s.sparkoperator.v1beta2.models.V1beta2SparkApplicationSpecSparkUIOptions;
import io.swagger.annotations.ApiModel;
import io.swagger.annotations.ApiModelProperty;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * V1beta2SparkApplicationSpec
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2022-04-05T08:55:35.785Z[Etc/UTC]")
public class V1beta2SparkApplicationSpec {
  public static final String SERIALIZED_NAME_ARGUMENTS = "arguments";
  @SerializedName(SERIALIZED_NAME_ARGUMENTS)
  private List<String> arguments = null;

  public static final String SERIALIZED_NAME_BATCH_SCHEDULER = "batchScheduler";
  @SerializedName(SERIALIZED_NAME_BATCH_SCHEDULER)
  private String batchScheduler;

  public static final String SERIALIZED_NAME_BATCH_SCHEDULER_OPTIONS = "batchSchedulerOptions";
  @SerializedName(SERIALIZED_NAME_BATCH_SCHEDULER_OPTIONS)
  private V1beta2ScheduledSparkApplicationSpecTemplateBatchSchedulerOptions batchSchedulerOptions;

  public static final String SERIALIZED_NAME_DEPS = "deps";
  @SerializedName(SERIALIZED_NAME_DEPS)
  private V1beta2ScheduledSparkApplicationSpecTemplateDeps deps;

  public static final String SERIALIZED_NAME_DRIVER = "driver";
  @SerializedName(SERIALIZED_NAME_DRIVER)
  private V1beta2ScheduledSparkApplicationSpecTemplateDriver driver;

  public static final String SERIALIZED_NAME_DYNAMIC_ALLOCATION = "dynamicAllocation";
  @SerializedName(SERIALIZED_NAME_DYNAMIC_ALLOCATION)
  private V1beta2ScheduledSparkApplicationSpecTemplateDynamicAllocation dynamicAllocation;

  public static final String SERIALIZED_NAME_EXECUTOR = "executor";
  @SerializedName(SERIALIZED_NAME_EXECUTOR)
  private V1beta2ScheduledSparkApplicationSpecTemplateExecutor executor;

  public static final String SERIALIZED_NAME_FAILURE_RETRIES = "failureRetries";
  @SerializedName(SERIALIZED_NAME_FAILURE_RETRIES)
  private Integer failureRetries;

  public static final String SERIALIZED_NAME_HADOOP_CONF = "hadoopConf";
  @SerializedName(SERIALIZED_NAME_HADOOP_CONF)
  private Map<String, String> hadoopConf = null;

  public static final String SERIALIZED_NAME_HADOOP_CONFIG_MAP = "hadoopConfigMap";
  @SerializedName(SERIALIZED_NAME_HADOOP_CONFIG_MAP)
  private String hadoopConfigMap;

  public static final String SERIALIZED_NAME_IMAGE = "image";
  @SerializedName(SERIALIZED_NAME_IMAGE)
  private String image;

  public static final String SERIALIZED_NAME_IMAGE_PULL_POLICY = "imagePullPolicy";
  @SerializedName(SERIALIZED_NAME_IMAGE_PULL_POLICY)
  private String imagePullPolicy;

  public static final String SERIALIZED_NAME_IMAGE_PULL_SECRETS = "imagePullSecrets";
  @SerializedName(SERIALIZED_NAME_IMAGE_PULL_SECRETS)
  private List<String> imagePullSecrets = null;

  public static final String SERIALIZED_NAME_MAIN_APPLICATION_FILE = "mainApplicationFile";
  @SerializedName(SERIALIZED_NAME_MAIN_APPLICATION_FILE)
  private String mainApplicationFile;

  public static final String SERIALIZED_NAME_MAIN_CLASS = "mainClass";
  @SerializedName(SERIALIZED_NAME_MAIN_CLASS)
  private String mainClass;

  public static final String SERIALIZED_NAME_MEMORY_OVERHEAD_FACTOR = "memoryOverheadFactor";
  @SerializedName(SERIALIZED_NAME_MEMORY_OVERHEAD_FACTOR)
  private String memoryOverheadFactor;

  /**
   * Gets or Sets mode
   */
  @JsonAdapter(ModeEnum.Adapter.class)
  public enum ModeEnum {
    CLUSTER("cluster"),
    
    CLIENT("client");

    private String value;

    ModeEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static ModeEnum fromValue(String value) {
      for (ModeEnum b : ModeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<ModeEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final ModeEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public ModeEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return ModeEnum.fromValue(value);
      }
    }
  }

  public static final String SERIALIZED_NAME_MODE = "mode";
  @SerializedName(SERIALIZED_NAME_MODE)
  private ModeEnum mode;

  public static final String SERIALIZED_NAME_MONITORING = "monitoring";
  @SerializedName(SERIALIZED_NAME_MONITORING)
  private V1beta2ScheduledSparkApplicationSpecTemplateMonitoring monitoring;

  public static final String SERIALIZED_NAME_NODE_SELECTOR = "nodeSelector";
  @SerializedName(SERIALIZED_NAME_NODE_SELECTOR)
  private Map<String, String> nodeSelector = null;

  public static final String SERIALIZED_NAME_PROXY_USER = "proxyUser";
  @SerializedName(SERIALIZED_NAME_PROXY_USER)
  private String proxyUser;

  /**
   * Gets or Sets pythonVersion
   */
  @JsonAdapter(PythonVersionEnum.Adapter.class)
  public enum PythonVersionEnum {
    _2("2"),
    
    _3("3");

    private String value;

    PythonVersionEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static PythonVersionEnum fromValue(String value) {
      for (PythonVersionEnum b : PythonVersionEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<PythonVersionEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final PythonVersionEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public PythonVersionEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return PythonVersionEnum.fromValue(value);
      }
    }
  }

  public static final String SERIALIZED_NAME_PYTHON_VERSION = "pythonVersion";
  @SerializedName(SERIALIZED_NAME_PYTHON_VERSION)
  private PythonVersionEnum pythonVersion;

  public static final String SERIALIZED_NAME_RESTART_POLICY = "restartPolicy";
  @SerializedName(SERIALIZED_NAME_RESTART_POLICY)
  private V1beta2ScheduledSparkApplicationSpecTemplateRestartPolicy restartPolicy;

  public static final String SERIALIZED_NAME_RETRY_INTERVAL = "retryInterval";
  @SerializedName(SERIALIZED_NAME_RETRY_INTERVAL)
  private Long retryInterval;

  public static final String SERIALIZED_NAME_SPARK_CONF = "sparkConf";
  @SerializedName(SERIALIZED_NAME_SPARK_CONF)
  private Map<String, String> sparkConf = null;

  public static final String SERIALIZED_NAME_SPARK_CONFIG_MAP = "sparkConfigMap";
  @SerializedName(SERIALIZED_NAME_SPARK_CONFIG_MAP)
  private String sparkConfigMap;

  public static final String SERIALIZED_NAME_SPARK_U_I_OPTIONS = "sparkUIOptions";
  @SerializedName(SERIALIZED_NAME_SPARK_U_I_OPTIONS)
  private V1beta2SparkApplicationSpecSparkUIOptions sparkUIOptions;

  public static final String SERIALIZED_NAME_SPARK_VERSION = "sparkVersion";
  @SerializedName(SERIALIZED_NAME_SPARK_VERSION)
  private String sparkVersion;

  public static final String SERIALIZED_NAME_TIME_TO_LIVE_SECONDS = "timeToLiveSeconds";
  @SerializedName(SERIALIZED_NAME_TIME_TO_LIVE_SECONDS)
  private Long timeToLiveSeconds;

  /**
   * Gets or Sets type
   */
  @JsonAdapter(TypeEnum.Adapter.class)
  public enum TypeEnum {
    JAVA("Java"),
    
    PYTHON("Python"),
    
    SCALA("Scala"),
    
    R("R");

    private String value;

    TypeEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static TypeEnum fromValue(String value) {
      for (TypeEnum b : TypeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<TypeEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final TypeEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public TypeEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return TypeEnum.fromValue(value);
      }
    }
  }

  public static final String SERIALIZED_NAME_TYPE = "type";
  @SerializedName(SERIALIZED_NAME_TYPE)
  private TypeEnum type;

  public static final String SERIALIZED_NAME_VOLUMES = "volumes";
  @SerializedName(SERIALIZED_NAME_VOLUMES)
  private List<V1beta2ScheduledSparkApplicationSpecTemplateVolumes> volumes = null;


  public V1beta2SparkApplicationSpec arguments(List<String> arguments) {
    
    this.arguments = arguments;
    return this;
  }

  public V1beta2SparkApplicationSpec addArgumentsItem(String argumentsItem) {
    if (this.arguments == null) {
      this.arguments = new ArrayList<>();
    }
    this.arguments.add(argumentsItem);
    return this;
  }

   /**
   * Get arguments
   * @return arguments
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public List<String> getArguments() {
    return arguments;
  }


  public void setArguments(List<String> arguments) {
    this.arguments = arguments;
  }


  public V1beta2SparkApplicationSpec batchScheduler(String batchScheduler) {
    
    this.batchScheduler = batchScheduler;
    return this;
  }

   /**
   * Get batchScheduler
   * @return batchScheduler
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getBatchScheduler() {
    return batchScheduler;
  }


  public void setBatchScheduler(String batchScheduler) {
    this.batchScheduler = batchScheduler;
  }


  public V1beta2SparkApplicationSpec batchSchedulerOptions(V1beta2ScheduledSparkApplicationSpecTemplateBatchSchedulerOptions batchSchedulerOptions) {
    
    this.batchSchedulerOptions = batchSchedulerOptions;
    return this;
  }

   /**
   * Get batchSchedulerOptions
   * @return batchSchedulerOptions
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateBatchSchedulerOptions getBatchSchedulerOptions() {
    return batchSchedulerOptions;
  }


  public void setBatchSchedulerOptions(V1beta2ScheduledSparkApplicationSpecTemplateBatchSchedulerOptions batchSchedulerOptions) {
    this.batchSchedulerOptions = batchSchedulerOptions;
  }


  public V1beta2SparkApplicationSpec deps(V1beta2ScheduledSparkApplicationSpecTemplateDeps deps) {
    
    this.deps = deps;
    return this;
  }

   /**
   * Get deps
   * @return deps
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateDeps getDeps() {
    return deps;
  }


  public void setDeps(V1beta2ScheduledSparkApplicationSpecTemplateDeps deps) {
    this.deps = deps;
  }


  public V1beta2SparkApplicationSpec driver(V1beta2ScheduledSparkApplicationSpecTemplateDriver driver) {
    
    this.driver = driver;
    return this;
  }

   /**
   * Get driver
   * @return driver
  **/
  @ApiModelProperty(required = true, value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateDriver getDriver() {
    return driver;
  }


  public void setDriver(V1beta2ScheduledSparkApplicationSpecTemplateDriver driver) {
    this.driver = driver;
  }


  public V1beta2SparkApplicationSpec dynamicAllocation(V1beta2ScheduledSparkApplicationSpecTemplateDynamicAllocation dynamicAllocation) {
    
    this.dynamicAllocation = dynamicAllocation;
    return this;
  }

   /**
   * Get dynamicAllocation
   * @return dynamicAllocation
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateDynamicAllocation getDynamicAllocation() {
    return dynamicAllocation;
  }


  public void setDynamicAllocation(V1beta2ScheduledSparkApplicationSpecTemplateDynamicAllocation dynamicAllocation) {
    this.dynamicAllocation = dynamicAllocation;
  }


  public V1beta2SparkApplicationSpec executor(V1beta2ScheduledSparkApplicationSpecTemplateExecutor executor) {
    
    this.executor = executor;
    return this;
  }

   /**
   * Get executor
   * @return executor
  **/
  @ApiModelProperty(required = true, value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateExecutor getExecutor() {
    return executor;
  }


  public void setExecutor(V1beta2ScheduledSparkApplicationSpecTemplateExecutor executor) {
    this.executor = executor;
  }


  public V1beta2SparkApplicationSpec failureRetries(Integer failureRetries) {
    
    this.failureRetries = failureRetries;
    return this;
  }

   /**
   * Get failureRetries
   * @return failureRetries
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public Integer getFailureRetries() {
    return failureRetries;
  }


  public void setFailureRetries(Integer failureRetries) {
    this.failureRetries = failureRetries;
  }


  public V1beta2SparkApplicationSpec hadoopConf(Map<String, String> hadoopConf) {
    
    this.hadoopConf = hadoopConf;
    return this;
  }

  public V1beta2SparkApplicationSpec putHadoopConfItem(String key, String hadoopConfItem) {
    if (this.hadoopConf == null) {
      this.hadoopConf = new HashMap<>();
    }
    this.hadoopConf.put(key, hadoopConfItem);
    return this;
  }

   /**
   * Get hadoopConf
   * @return hadoopConf
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public Map<String, String> getHadoopConf() {
    return hadoopConf;
  }


  public void setHadoopConf(Map<String, String> hadoopConf) {
    this.hadoopConf = hadoopConf;
  }


  public V1beta2SparkApplicationSpec hadoopConfigMap(String hadoopConfigMap) {
    
    this.hadoopConfigMap = hadoopConfigMap;
    return this;
  }

   /**
   * Get hadoopConfigMap
   * @return hadoopConfigMap
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getHadoopConfigMap() {
    return hadoopConfigMap;
  }


  public void setHadoopConfigMap(String hadoopConfigMap) {
    this.hadoopConfigMap = hadoopConfigMap;
  }


  public V1beta2SparkApplicationSpec image(String image) {
    
    this.image = image;
    return this;
  }

   /**
   * Get image
   * @return image
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getImage() {
    return image;
  }


  public void setImage(String image) {
    this.image = image;
  }


  public V1beta2SparkApplicationSpec imagePullPolicy(String imagePullPolicy) {
    
    this.imagePullPolicy = imagePullPolicy;
    return this;
  }

   /**
   * Get imagePullPolicy
   * @return imagePullPolicy
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getImagePullPolicy() {
    return imagePullPolicy;
  }


  public void setImagePullPolicy(String imagePullPolicy) {
    this.imagePullPolicy = imagePullPolicy;
  }


  public V1beta2SparkApplicationSpec imagePullSecrets(List<String> imagePullSecrets) {
    
    this.imagePullSecrets = imagePullSecrets;
    return this;
  }

  public V1beta2SparkApplicationSpec addImagePullSecretsItem(String imagePullSecretsItem) {
    if (this.imagePullSecrets == null) {
      this.imagePullSecrets = new ArrayList<>();
    }
    this.imagePullSecrets.add(imagePullSecretsItem);
    return this;
  }

   /**
   * Get imagePullSecrets
   * @return imagePullSecrets
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public List<String> getImagePullSecrets() {
    return imagePullSecrets;
  }


  public void setImagePullSecrets(List<String> imagePullSecrets) {
    this.imagePullSecrets = imagePullSecrets;
  }


  public V1beta2SparkApplicationSpec mainApplicationFile(String mainApplicationFile) {
    
    this.mainApplicationFile = mainApplicationFile;
    return this;
  }

   /**
   * Get mainApplicationFile
   * @return mainApplicationFile
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getMainApplicationFile() {
    return mainApplicationFile;
  }


  public void setMainApplicationFile(String mainApplicationFile) {
    this.mainApplicationFile = mainApplicationFile;
  }


  public V1beta2SparkApplicationSpec mainClass(String mainClass) {
    
    this.mainClass = mainClass;
    return this;
  }

   /**
   * Get mainClass
   * @return mainClass
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getMainClass() {
    return mainClass;
  }


  public void setMainClass(String mainClass) {
    this.mainClass = mainClass;
  }


  public V1beta2SparkApplicationSpec memoryOverheadFactor(String memoryOverheadFactor) {
    
    this.memoryOverheadFactor = memoryOverheadFactor;
    return this;
  }

   /**
   * Get memoryOverheadFactor
   * @return memoryOverheadFactor
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getMemoryOverheadFactor() {
    return memoryOverheadFactor;
  }


  public void setMemoryOverheadFactor(String memoryOverheadFactor) {
    this.memoryOverheadFactor = memoryOverheadFactor;
  }


  public V1beta2SparkApplicationSpec mode(ModeEnum mode) {
    
    this.mode = mode;
    return this;
  }

   /**
   * Get mode
   * @return mode
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public ModeEnum getMode() {
    return mode;
  }


  public void setMode(ModeEnum mode) {
    this.mode = mode;
  }


  public V1beta2SparkApplicationSpec monitoring(V1beta2ScheduledSparkApplicationSpecTemplateMonitoring monitoring) {
    
    this.monitoring = monitoring;
    return this;
  }

   /**
   * Get monitoring
   * @return monitoring
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateMonitoring getMonitoring() {
    return monitoring;
  }


  public void setMonitoring(V1beta2ScheduledSparkApplicationSpecTemplateMonitoring monitoring) {
    this.monitoring = monitoring;
  }


  public V1beta2SparkApplicationSpec nodeSelector(Map<String, String> nodeSelector) {
    
    this.nodeSelector = nodeSelector;
    return this;
  }

  public V1beta2SparkApplicationSpec putNodeSelectorItem(String key, String nodeSelectorItem) {
    if (this.nodeSelector == null) {
      this.nodeSelector = new HashMap<>();
    }
    this.nodeSelector.put(key, nodeSelectorItem);
    return this;
  }

   /**
   * Get nodeSelector
   * @return nodeSelector
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public Map<String, String> getNodeSelector() {
    return nodeSelector;
  }


  public void setNodeSelector(Map<String, String> nodeSelector) {
    this.nodeSelector = nodeSelector;
  }


  public V1beta2SparkApplicationSpec proxyUser(String proxyUser) {
    
    this.proxyUser = proxyUser;
    return this;
  }

   /**
   * Get proxyUser
   * @return proxyUser
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getProxyUser() {
    return proxyUser;
  }


  public void setProxyUser(String proxyUser) {
    this.proxyUser = proxyUser;
  }


  public V1beta2SparkApplicationSpec pythonVersion(PythonVersionEnum pythonVersion) {
    
    this.pythonVersion = pythonVersion;
    return this;
  }

   /**
   * Get pythonVersion
   * @return pythonVersion
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public PythonVersionEnum getPythonVersion() {
    return pythonVersion;
  }


  public void setPythonVersion(PythonVersionEnum pythonVersion) {
    this.pythonVersion = pythonVersion;
  }


  public V1beta2SparkApplicationSpec restartPolicy(V1beta2ScheduledSparkApplicationSpecTemplateRestartPolicy restartPolicy) {
    
    this.restartPolicy = restartPolicy;
    return this;
  }

   /**
   * Get restartPolicy
   * @return restartPolicy
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public V1beta2ScheduledSparkApplicationSpecTemplateRestartPolicy getRestartPolicy() {
    return restartPolicy;
  }


  public void setRestartPolicy(V1beta2ScheduledSparkApplicationSpecTemplateRestartPolicy restartPolicy) {
    this.restartPolicy = restartPolicy;
  }


  public V1beta2SparkApplicationSpec retryInterval(Long retryInterval) {
    
    this.retryInterval = retryInterval;
    return this;
  }

   /**
   * Get retryInterval
   * @return retryInterval
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public Long getRetryInterval() {
    return retryInterval;
  }


  public void setRetryInterval(Long retryInterval) {
    this.retryInterval = retryInterval;
  }


  public V1beta2SparkApplicationSpec sparkConf(Map<String, String> sparkConf) {
    
    this.sparkConf = sparkConf;
    return this;
  }

  public V1beta2SparkApplicationSpec putSparkConfItem(String key, String sparkConfItem) {
    if (this.sparkConf == null) {
      this.sparkConf = new HashMap<>();
    }
    this.sparkConf.put(key, sparkConfItem);
    return this;
  }

   /**
   * Get sparkConf
   * @return sparkConf
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public Map<String, String> getSparkConf() {
    return sparkConf;
  }


  public void setSparkConf(Map<String, String> sparkConf) {
    this.sparkConf = sparkConf;
  }


  public V1beta2SparkApplicationSpec sparkConfigMap(String sparkConfigMap) {
    
    this.sparkConfigMap = sparkConfigMap;
    return this;
  }

   /**
   * Get sparkConfigMap
   * @return sparkConfigMap
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public String getSparkConfigMap() {
    return sparkConfigMap;
  }


  public void setSparkConfigMap(String sparkConfigMap) {
    this.sparkConfigMap = sparkConfigMap;
  }


  public V1beta2SparkApplicationSpec sparkUIOptions(V1beta2SparkApplicationSpecSparkUIOptions sparkUIOptions) {
    
    this.sparkUIOptions = sparkUIOptions;
    return this;
  }

   /**
   * Get sparkUIOptions
   * @return sparkUIOptions
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public V1beta2SparkApplicationSpecSparkUIOptions getSparkUIOptions() {
    return sparkUIOptions;
  }


  public void setSparkUIOptions(V1beta2SparkApplicationSpecSparkUIOptions sparkUIOptions) {
    this.sparkUIOptions = sparkUIOptions;
  }


  public V1beta2SparkApplicationSpec sparkVersion(String sparkVersion) {
    
    this.sparkVersion = sparkVersion;
    return this;
  }

   /**
   * Get sparkVersion
   * @return sparkVersion
  **/
  @ApiModelProperty(required = true, value = "")

  public String getSparkVersion() {
    return sparkVersion;
  }


  public void setSparkVersion(String sparkVersion) {
    this.sparkVersion = sparkVersion;
  }


  public V1beta2SparkApplicationSpec timeToLiveSeconds(Long timeToLiveSeconds) {
    
    this.timeToLiveSeconds = timeToLiveSeconds;
    return this;
  }

   /**
   * Get timeToLiveSeconds
   * @return timeToLiveSeconds
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public Long getTimeToLiveSeconds() {
    return timeToLiveSeconds;
  }


  public void setTimeToLiveSeconds(Long timeToLiveSeconds) {
    this.timeToLiveSeconds = timeToLiveSeconds;
  }


  public V1beta2SparkApplicationSpec type(TypeEnum type) {
    
    this.type = type;
    return this;
  }

   /**
   * Get type
   * @return type
  **/
  @ApiModelProperty(required = true, value = "")

  public TypeEnum getType() {
    return type;
  }


  public void setType(TypeEnum type) {
    this.type = type;
  }


  public V1beta2SparkApplicationSpec volumes(List<V1beta2ScheduledSparkApplicationSpecTemplateVolumes> volumes) {
    
    this.volumes = volumes;
    return this;
  }

  public V1beta2SparkApplicationSpec addVolumesItem(V1beta2ScheduledSparkApplicationSpecTemplateVolumes volumesItem) {
    if (this.volumes == null) {
      this.volumes = new ArrayList<>();
    }
    this.volumes.add(volumesItem);
    return this;
  }

   /**
   * Get volumes
   * @return volumes
  **/
  @javax.annotation.Nullable
  @ApiModelProperty(value = "")

  public List<V1beta2ScheduledSparkApplicationSpecTemplateVolumes> getVolumes() {
    return volumes;
  }


  public void setVolumes(List<V1beta2ScheduledSparkApplicationSpecTemplateVolumes> volumes) {
    this.volumes = volumes;
  }


  @Override
  public boolean equals(java.lang.Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    V1beta2SparkApplicationSpec v1beta2SparkApplicationSpec = (V1beta2SparkApplicationSpec) o;
    return Objects.equals(this.arguments, v1beta2SparkApplicationSpec.arguments) &&
        Objects.equals(this.batchScheduler, v1beta2SparkApplicationSpec.batchScheduler) &&
        Objects.equals(this.batchSchedulerOptions, v1beta2SparkApplicationSpec.batchSchedulerOptions) &&
        Objects.equals(this.deps, v1beta2SparkApplicationSpec.deps) &&
        Objects.equals(this.driver, v1beta2SparkApplicationSpec.driver) &&
        Objects.equals(this.dynamicAllocation, v1beta2SparkApplicationSpec.dynamicAllocation) &&
        Objects.equals(this.executor, v1beta2SparkApplicationSpec.executor) &&
        Objects.equals(this.failureRetries, v1beta2SparkApplicationSpec.failureRetries) &&
        Objects.equals(this.hadoopConf, v1beta2SparkApplicationSpec.hadoopConf) &&
        Objects.equals(this.hadoopConfigMap, v1beta2SparkApplicationSpec.hadoopConfigMap) &&
        Objects.equals(this.image, v1beta2SparkApplicationSpec.image) &&
        Objects.equals(this.imagePullPolicy, v1beta2SparkApplicationSpec.imagePullPolicy) &&
        Objects.equals(this.imagePullSecrets, v1beta2SparkApplicationSpec.imagePullSecrets) &&
        Objects.equals(this.mainApplicationFile, v1beta2SparkApplicationSpec.mainApplicationFile) &&
        Objects.equals(this.mainClass, v1beta2SparkApplicationSpec.mainClass) &&
        Objects.equals(this.memoryOverheadFactor, v1beta2SparkApplicationSpec.memoryOverheadFactor) &&
        Objects.equals(this.mode, v1beta2SparkApplicationSpec.mode) &&
        Objects.equals(this.monitoring, v1beta2SparkApplicationSpec.monitoring) &&
        Objects.equals(this.nodeSelector, v1beta2SparkApplicationSpec.nodeSelector) &&
        Objects.equals(this.proxyUser, v1beta2SparkApplicationSpec.proxyUser) &&
        Objects.equals(this.pythonVersion, v1beta2SparkApplicationSpec.pythonVersion) &&
        Objects.equals(this.restartPolicy, v1beta2SparkApplicationSpec.restartPolicy) &&
        Objects.equals(this.retryInterval, v1beta2SparkApplicationSpec.retryInterval) &&
        Objects.equals(this.sparkConf, v1beta2SparkApplicationSpec.sparkConf) &&
        Objects.equals(this.sparkConfigMap, v1beta2SparkApplicationSpec.sparkConfigMap) &&
        Objects.equals(this.sparkUIOptions, v1beta2SparkApplicationSpec.sparkUIOptions) &&
        Objects.equals(this.sparkVersion, v1beta2SparkApplicationSpec.sparkVersion) &&
        Objects.equals(this.timeToLiveSeconds, v1beta2SparkApplicationSpec.timeToLiveSeconds) &&
        Objects.equals(this.type, v1beta2SparkApplicationSpec.type) &&
        Objects.equals(this.volumes, v1beta2SparkApplicationSpec.volumes);
  }

  @Override
  public int hashCode() {
    return Objects.hash(arguments, batchScheduler, batchSchedulerOptions, deps, driver, dynamicAllocation, executor, failureRetries, hadoopConf, hadoopConfigMap, image, imagePullPolicy, imagePullSecrets, mainApplicationFile, mainClass, memoryOverheadFactor, mode, monitoring, nodeSelector, proxyUser, pythonVersion, restartPolicy, retryInterval, sparkConf, sparkConfigMap, sparkUIOptions, sparkVersion, timeToLiveSeconds, type, volumes);
  }


  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class V1beta2SparkApplicationSpec {\n");
    sb.append("    arguments: ").append(toIndentedString(arguments)).append("\n");
    sb.append("    batchScheduler: ").append(toIndentedString(batchScheduler)).append("\n");
    sb.append("    batchSchedulerOptions: ").append(toIndentedString(batchSchedulerOptions)).append("\n");
    sb.append("    deps: ").append(toIndentedString(deps)).append("\n");
    sb.append("    driver: ").append(toIndentedString(driver)).append("\n");
    sb.append("    dynamicAllocation: ").append(toIndentedString(dynamicAllocation)).append("\n");
    sb.append("    executor: ").append(toIndentedString(executor)).append("\n");
    sb.append("    failureRetries: ").append(toIndentedString(failureRetries)).append("\n");
    sb.append("    hadoopConf: ").append(toIndentedString(hadoopConf)).append("\n");
    sb.append("    hadoopConfigMap: ").append(toIndentedString(hadoopConfigMap)).append("\n");
    sb.append("    image: ").append(toIndentedString(image)).append("\n");
    sb.append("    imagePullPolicy: ").append(toIndentedString(imagePullPolicy)).append("\n");
    sb.append("    imagePullSecrets: ").append(toIndentedString(imagePullSecrets)).append("\n");
    sb.append("    mainApplicationFile: ").append(toIndentedString(mainApplicationFile)).append("\n");
    sb.append("    mainClass: ").append(toIndentedString(mainClass)).append("\n");
    sb.append("    memoryOverheadFactor: ").append(toIndentedString(memoryOverheadFactor)).append("\n");
    sb.append("    mode: ").append(toIndentedString(mode)).append("\n");
    sb.append("    monitoring: ").append(toIndentedString(monitoring)).append("\n");
    sb.append("    nodeSelector: ").append(toIndentedString(nodeSelector)).append("\n");
    sb.append("    proxyUser: ").append(toIndentedString(proxyUser)).append("\n");
    sb.append("    pythonVersion: ").append(toIndentedString(pythonVersion)).append("\n");
    sb.append("    restartPolicy: ").append(toIndentedString(restartPolicy)).append("\n");
    sb.append("    retryInterval: ").append(toIndentedString(retryInterval)).append("\n");
    sb.append("    sparkConf: ").append(toIndentedString(sparkConf)).append("\n");
    sb.append("    sparkConfigMap: ").append(toIndentedString(sparkConfigMap)).append("\n");
    sb.append("    sparkUIOptions: ").append(toIndentedString(sparkUIOptions)).append("\n");
    sb.append("    sparkVersion: ").append(toIndentedString(sparkVersion)).append("\n");
    sb.append("    timeToLiveSeconds: ").append(toIndentedString(timeToLiveSeconds)).append("\n");
    sb.append("    type: ").append(toIndentedString(type)).append("\n");
    sb.append("    volumes: ").append(toIndentedString(volumes)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(java.lang.Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }

}

